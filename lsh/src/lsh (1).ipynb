{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "yPFGg5UMZ-g3",
        "outputId": "1ba7dd1a-e7f9-4028-c554-6084d8f07670"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "cp: cannot stat 'drive/MyDrive/MMDS-data/spark-3.1.1-bin-hadoop3.2.tgz': No such file or directory\n"
          ]
        }
      ],
      "source": [
        "!apt-get install openjdk-8-jdk-headless -qq > /dev/null\n",
        "!wget -q http://archive.apache.org/dist/spark/spark-3.1.1/spark-3.1.1-bin-hadoop3.2.tgz\n",
        "!cp drive/MyDrive/MMDS-data/spark-3.1.1-bin-hadoop3.2.tgz .\n",
        "!tar xf spark-3.1.1-bin-hadoop3.2.tgz\n",
        "!pip install -q findspark"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "jh-XaXqNu_zS"
      },
      "outputs": [],
      "source": [
        "import os\n",
        "os.environ[\"JAVA_HOME\"] = \"/usr/lib/jvm/java-8-openjdk-amd64\"\n",
        "os.environ[\"SPARK_HOME\"] = \"/content/spark-3.1.1-bin-hadoop3.2\""
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "CUQHn0gHvEpK"
      },
      "outputs": [],
      "source": [
        "import findspark\n",
        "findspark.init()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "0EF-wIHRyoFc"
      },
      "outputs": [],
      "source": [
        "import pyspark as spark\n",
        "from pyspark import SparkContext\n",
        "from pyspark.sql import SQLContext\n",
        "from pyspark.sql.functions import udf, col, lower, regexp_replace , monotonically_increasing_id\n",
        "from pyspark.ml.linalg import SparseVector\n",
        "from pyspark.sql.types import StructType, StructField, StringType , ArrayType ,IntegerType\n",
        "\n",
        "from pyspark.ml.linalg import SparseVector\n",
        "from pyspark.sql import SparkSession\n",
        "from pyspark.sql.functions import col, collect_set ,array_distinct , explode,when  , array_contains ,collect_list,  udf\n",
        "import random\n",
        "import hashlib\n",
        "import sys\n",
        "import itertools\n",
        "import time\n",
        "\n",
        "sc = SparkContext('local', 'First App')\n",
        "sc = SparkContext.getOrCreate('local')\n",
        "spark = SQLContext(sc)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Ju1WuMW1m8M_",
        "outputId": "c9bb5051-1cf2-446e-b4ae-8ca7d7b29f6d"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Execution time: -1176.1602883338928 seconds\n",
            "Execution time: -14.308485507965088 seconds\n",
            "Execution time: -91.87277722358704 seconds\n",
            "+-----+------------------+\n",
            "|docid|          distance|\n",
            "+-----+------------------+\n",
            "|    3|               1.0|\n",
            "|   15|0.5466666666666666|\n",
            "|   29|0.5128205128205128|\n",
            "|    1|               0.5|\n",
            "|   40|               0.5|\n",
            "+-----+------------------+\n",
            "\n"
          ]
        }
      ],
      "source": [
        "class LargDataMinHashLSH:\n",
        "    def __init__(self, path):\n",
        "        self.documents = None\n",
        "        self.path =  path\n",
        "        self.kshingles = 8\n",
        "        self.shingles = None\n",
        "        self.numberOfVectorSignature = 100\n",
        "        self.lensignatrue = None\n",
        "        self.df_shingles = None\n",
        "        self.shingle_ = None\n",
        "        self.hash_func = None\n",
        "        self.signatures = None\n",
        "        self.vectorboolean = None\n",
        "\n",
        "\n",
        "    def shingling(self, dataframe):\n",
        "      k = self.kshingles\n",
        "      rdd = dataframe.rdd\n",
        "\n",
        "      # Tạo RDD chứa các shingles\n",
        "      shingles_rdd = rdd.map(lambda shingle: (shingle[0], [shingle[1][i:i+k] for i in range(len(shingle[1]) - k + 1)]))\n",
        "      shingles_rdd_beforeJoin = shingles_rdd.map( lambda x : (0 , (str(x[0]) , x[1])))\n",
        "\n",
        "\n",
        "      # Lấy các shingle duy nhất\n",
        "      unique_shingles_rdd = shingles_rdd \\\n",
        "                  .flatMap(lambda x: x[1]) \\\n",
        "                  .distinct() \\\n",
        "                  .map( lambda x : ( 0 , [x])) \\\n",
        "                  .sortBy(lambda x: x[1])\\\n",
        "                  .reduceByKey( lambda x , y :  x +  y)\n",
        "\n",
        "      self.shingle_  = unique_shingles_rdd\n",
        "      # Kết hợp các RDD\n",
        "      joined_rdd = shingles_rdd_beforeJoin.join(unique_shingles_rdd)\n",
        "      # Tạo DataFrame từ kết quả\n",
        "\n",
        "      result_rdd = joined_rdd.map(lambda x: (x[1][0][0],   [1 if x[1][1][i] in x[1][0][1] else 0 for i in range(len(x[1][1]))]))\n",
        "      selected_df =result_rdd.toDF([\"docid\", \"features\"])\n",
        "      return selected_df\n",
        "\n",
        "    def minhashing(self  ,  df) :\n",
        "      count = len(self.shingle_.first()[1])\n",
        "      n = 100\n",
        "      # Tạo một danh sách hoán vị ngẫu nhiên cho mỗi hoán vị\n",
        "      permutations  = sc.parallelize(range(0 , n))\\\n",
        "        .map(lambda x: ( 0  , [random.sample(list(range(1, count + 1)), count)])) \\\n",
        "        .reduceByKey(lambda x , y :  x+y)\n",
        "\n",
        "      self.hash_func = permutations\n",
        "\n",
        "      # Kết hợp DataFrame với danh sách hoán vị\n",
        "      joined_rdd =  df.rdd.cartesian(permutations)\n",
        "      joined_rdd =   joined_rdd.map(lambda x :   (x[0][0] , [ min(perm_val for perm_val, row_val in zip(perm, x[0][1]) if row_val) for perm in x[1][1]]))\n",
        "      self.signatures = joined_rdd\n",
        "      signatures_df = spark.createDataFrame(joined_rdd , schema=[\"docid\" , \"hashes\"])\n",
        "      return signatures_df\n",
        "\n",
        "    def locality_sensitivity_hashing(self, df):\n",
        "        #Tạo các band\n",
        "        signature = df.rdd\n",
        "        b=20 # Số band\n",
        "        r=5 # Số hàng trong mỗi band\n",
        "\n",
        "        # create band\n",
        "        band_lsh =  signature.map(lambda band :  (band[0] , [[band[1][i] for i in range(j* r , j*r  + r) ] for j in range(b)]))\n",
        "\n",
        "        # hash band\n",
        "        hashed_bands = band_lsh.flatMap(lambda band: [(minHash(tuple(band[1][j]), j), band[0]) for j in range(b)])\n",
        "\n",
        "        # build bucket\n",
        "        buckets = hashed_bands.map(lambda bucket : (bucket[0] , bucket[1]))\\\n",
        "                              .reduceByKey(lambda doc1 , doc2 : doc1 +  doc2)\\\n",
        "                              .filter(lambda collision : len(collision [1]) > 1)\n",
        "\n",
        "        # Chuyển đổi kết quả thành DataFrame\n",
        "        res = buckets.flatMap(lambda x : [( i,x[0]  )for i in x[1]])\n",
        "        res_df = spark.createDataFrame(res , schema=[\"iddoc\"  , \"bucket_id\"])\n",
        "        return res_df\n",
        "\n",
        "    def approxNearestNeighbors(self , key : str , n :int):\n",
        "        \"\"\"\n",
        "            step 1 : convert str -> boolean -> signature\n",
        "            step 2 : loop each documents  distance item with each document use Jaccard\n",
        "            step 3 : take(n)\n",
        "        \"\"\"\n",
        "        # Bước 1: Chuyển đổi chuỗi thành vector boolean và chữ ký\n",
        "        # shingles_rdd = self.df_shingles.rdd\n",
        "        k = self.kshingles\n",
        "        union_shingle = self.shingle_\n",
        "        shingles_key = [(0 , [key[i:i+k]]) for i in range(len(key) - k + 1)]\n",
        "        querry_doc_rdd = sc.parallelize(shingles_key).reduceByKey(lambda x, y: x + y)\n",
        "        joined_rdd =  querry_doc_rdd.cartesian(union_shingle)\n",
        "        querry_doc_rdd = joined_rdd.map(lambda x: (0 , [1 if x[1][1][i] in x[0][1] else 0 for i in range(len(x[1][1]))]))\n",
        "\n",
        "        # Chuyển đổi thành (signature)\n",
        "        permutations = self.hash_func\n",
        "        joined_rdd =  querry_doc_rdd.cartesian(permutations)\n",
        "        querry_doc_rdd_signature =   joined_rdd.map(lambda x :   (x[0][0] , [ min(perm_val for perm_val, row_val in zip(perm, x[0][1]) if row_val) for perm in x[1][1]]))\n",
        "        # querry_doc_rdd_signature.toDF().show(truncate = False)\n",
        "\n",
        "        # Tính Jaccard similarity cho mỗi cặp\n",
        "        # cartesian_product = shingles_rdd.cartesian(querry_doc_rdd_signature)\n",
        "\n",
        "        signatures =  self.signatures\n",
        "        cartesian_product = signatures.cartesian(querry_doc_rdd_signature)\n",
        "        # cartesian_product.toDF().show(truncate=False)\n",
        "\n",
        "        #Compute Jaccard similarity for each pair\n",
        "        jaccard = cartesian_product.map(lambda x : (  x[0][0]  , float(len(set(x[0][1]).intersection(x[1][1]))) /  float(len(set(x[0][1]).union(x[1][1]))) ))\\\n",
        "                                  .sortBy(lambda x : x[1] , ascending= False)\\\n",
        "                                  .take(n)\n",
        "\n",
        "        # Tạo DataFrame từ kết quả\n",
        "        jaccard_ok =  spark.createDataFrame(jaccard , schema=[\"docid\", \"distance\"])\n",
        "        jaccard_ok.show()\n",
        "\n",
        "\n",
        "    def run(self):\n",
        "        self.documents = loadDataFrame(self.path)\n",
        "        start_time = time.time()\n",
        "        step1  = self.shingling(self.documents)\n",
        "        # step1.show(10 , truncate = False)\n",
        "        end_time = time.time()\n",
        "        print(f\"Execution time: { start_time -  end_time} seconds\")\n",
        "\n",
        "        start_time = time.time()\n",
        "        step2 = self.minhashing(step1)\n",
        "        # step2.show(10  ,truncate = False)\n",
        "        end_time = time.time()\n",
        "        print(f\"Execution time: { start_time -  end_time} seconds\")\n",
        "\n",
        "        start_time = time.time()\n",
        "        step3 = self.locality_sensitivity_hashing(step2)\n",
        "        # step3.show(10)\n",
        "        end_time = time.time()\n",
        "        print(f\"Execution time: { start_time -  end_time} seconds\")\n",
        "\n",
        "def loadDataFrame(path: str):\n",
        "    try:\n",
        "        spark = SparkSession.builder.appName(\"LSH\").getOrCreate()\n",
        "        rdd = sc.textFile(path)\n",
        "        schema = StructType([\n",
        "            StructField(\"id\", StringType(), nullable=True),\n",
        "            StructField(\"features\", StringType(), nullable=True)\n",
        "        ])\n",
        "        df = spark.createDataFrame(rdd.zipWithIndex().map(lambda line: (str(line[1]) , line[0])) , schema=schema)\n",
        "        return df\n",
        "    except Exception as e:\n",
        "        print('Error:', e)\n",
        "        return None\n",
        "\n",
        "def hashFamily(i):\n",
        "  resultSize = 8\n",
        "  # how many bytes we want back\n",
        "  maxLen = 20\n",
        "  # how long can our i be (in decimal)\n",
        "  salt = str(i).zfill(maxLen)[-maxLen:]\n",
        "  def hashMember(x):\n",
        "    sequence = x + salt\n",
        "    return hashlib.sha1(sequence.encode(\"utf-8\")).digest()[-resultSize:]\n",
        "  return hashMember\n",
        "\n",
        "#Given a list of shingles, compute the hash for every shingle and keep the smallest value\n",
        "def minHash(shingles:list, i:int) -> int:\n",
        "  hash_fn = hashFamily(i)\n",
        "  minSignature = float('inf') #big number\n",
        "  for s in shingles:\n",
        "    #some casts are needed\n",
        "    hashSignature = int.from_bytes(hash_fn(str(s)), sys.byteorder)\n",
        "    if hashSignature < minSignature:\n",
        "        minSignature = hashSignature\n",
        "  return minSignature\n",
        "\n",
        "#Get the pairs from a list\n",
        "def get_pairs(collisions_list:list) -> tuple:\n",
        "  pair_list=[]\n",
        "  for pair in itertools.combinations(collisions_list,2):\n",
        "    pair_list.append(pair)\n",
        "  return tuple(pair_list)\n",
        "\n",
        "\n",
        "path = '/content/WebOfScience-50.txt'\n",
        "lsh = LargDataMinHashLSH(path)\n",
        "lsh.run()\n",
        "\n",
        "\n",
        "querry = '1,2-Dichloropropane (1,2-DCP) and dichloromethane (DCM) are possible causative agents associated with the development of cholangiocarcinoma in employees working in printing plant in Osaka, Japan. However, few reports have demonstrated an association between these agents and cholangiocarcinoma in rodent carcinogenicity studies. Moreover, the combined effects of these compounds have not been fully elucidated. In the present study, we evaluated the in vivo mutagenicity of 1,2-DCP and DCM, alone or combined, in the livers of gpt delta rats. Six-week-old male F344 gpt delta rats were treated with 1,2-DCP, DCM or 1,2-DCP+DCM by oral administration for 4weeks at the dose (200mgkg(-1) body weight 1,2-DCP and 500mgkg(-1) body weight DCM) used in the carcinogenesis study performed by the National Toxicology Program. In vivo mutagenicity was analyzed by gpt mutation/Spi(-) assays in the livers of rats. In addition, gene and protein expression of CYP2E1 and GSTT1, the major enzymes responsible for the genotoxic effects of 1,2-DCP and DCM, were analyzed by quantitative polymerase chain reaction and western blotting. Gpt and Spi(-) mutation frequencies were not increased by 1,2-DCP and/or DCM in any group. Additionally, there were no significant changes in the gene and protein expression of CYP2E1 and GSTT1 in any group. These results indicated that 1,2-DCP, DCM and 1,2-DCP+DCM had no significant impact on mutagenicity in the livers of gpt delta rats under our experimental conditions. Copyright (c) 2016 John Wiley & Sons, Ltd.'\n",
        "lsh.approxNearestNeighbors(querry  , 5)\n"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4"
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}